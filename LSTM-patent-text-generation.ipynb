{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "This demo explores how long short-term memory (LSTM) units of a neural network can be used to generate sequence data such as text. We try to learn the latent space of specific type of language model, patent descriptions, and train our neural network to predict the next character of a text sequence drawn from this probabilistic space. When we apply this process iteratively, we can generate completely new patent descriptions from an initial seed phrase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Make sure that you have Python 3.+ installed.\n",
    "This demo uses Keras, a high-level deep-learning framework with TensorFlow backend.\n",
    "\n",
    "If you have not setup Keras on your machine, first, install Tensorflow from terminal:\n",
    "`pip install tensorflow`\n",
    "\n",
    "And Keras next:\n",
    "`pip install keras`\n",
    "\n",
    "This demo uses also the following dependencies: **wget numpy zipfile tqdm**\n",
    "To make sure you have these, run\n",
    "\n",
    "`pip install wget numpy zipfile tqdm`\n",
    "\n",
    "__Note: You may have to reopen this notebook after installing packages.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "First, we get our patent data from USPTO database (http://www.patentsview.org/download/). For this case, we use the brief summary dataset (13.51 GB).\n",
    "\n",
    "**Note: This is a large text dataset (>50 GB).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "zip_path = './brf_sum_text.tsv.zip'\n",
    "if not os.path.isfile(zip_path):\n",
    "    print('Downloading dataset ')\n",
    "    url = 'http://s3.amazonaws.com/data-patentsview-org/20180528/download/brf_sum_text.tsv.zip'\n",
    "    zip_path = wget.download(url)\n",
    "    print('')\n",
    "    print('File downloaded: ' + zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Next, we need to unzip the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfolders(path_to_parent):\n",
    "     try:\n",
    "        return next(os.walk(path_to_parent))[1]\n",
    "     except StopIteration:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset file: ./data/20180528/bulk-downloads/brf_sum_text.tsv\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "if not os.path.isdir('./data'):\n",
    "    print('unzipping...')\n",
    "    with zipfile.ZipFile(zip_path , 'r') as zip_ref:\n",
    "        zip_ref.extractall(base_dir)\n",
    "    print('done')\n",
    "    \n",
    "text_dir = subfolders('./data')[0]\n",
    "text_file = './data/' + text_dir + '/bulk-downloads/brf_sum_text.tsv'\n",
    "\n",
    "print('Dataset file: ' + text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practical reasons we only read the first 500 lines of the full patent description dataset. This should give us around 1 million characters which is enough text to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 14288.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text length: 947785\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "new_text = []\n",
    "read_lines = 100\n",
    "with open(text_file, 'r', encoding='utf8') as f:\n",
    "    for i in tqdm(range(read_lines)):\n",
    "        new_text.append(f.readline())\n",
    "        \n",
    "text = ''.join(new_text)\n",
    "print('Total text length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 315909\n",
      "Unique characters: 126\n",
      "['\\t', '\\n', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '§', '©', '®', '°', '·', '½', '×', 'é', 'Ē', 'Δ', 'α', 'β', 'γ', 'ε', 'θ', 'κ', 'λ', 'μ', 'π', 'τ', '\\u2003', '–', '—', '‘', '’', '“', '”', '′', '″', 'Å', '→', '−', '≡', '≦', '═']\n",
      "Vectorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "315909it [00:05, 56030.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "## This part of code is copied from 2017 François Chollet:\n",
    "## https://github.com/fchollet/deep-learning-with-python-notebooks/\n",
    "\n",
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "print(chars)\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in tqdm(enumerate(sentences)):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "Here we use two layers of `LSTM` with 256 hidden units each, `dropout` layer and a final `softmax` activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(256, return_sequences=True ,input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimizer, we use `adam` with `categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "training...\n",
      "Seed text: \"about 6. Ambient temperatures or below (to just above freezi\"\n",
      "about 6. Ambient temperatures or below (to just above freezion on siciclentuen in merecting the condent er he for porine pectacter in anede (IN ar pelate an the are the roplor of ar the an of the the the ar and in mation the ang if whe the the ancing an the sement an are af the coster be the and in an the the of the mored an at an at en an the the a of the mate menal on fpered cores the the coted the purecod she the on ame encatee the conged are cons of a \n",
      "epoch 2\n",
      "training...\n",
      "Seed text: \"no longer functions to wipe out the ink. SUMMARY OF THE INVE\"\n",
      "no longer functions to wipe out the ink. SUMMARY OF THE INVE0, 25 15000, or .0 the to conding the prepent of the preses in the conterment appline of the proventi sigcher of the prosent of the preferent of the store the statent the sistor a deteration to mawe inclate procent for a sigrtally and the presed in the preser and the prection of the secon the sibe with the tepere the such seched the prefering the stacting a conces in the isster in the comber and a\n",
      "epoch 3\n",
      "training...\n",
      "Seed text: \"ction media being kept constantly basic (pH=9-11) and procee\"\n",
      "ction media being kept constantly basic (pH=9-11) and proceeration for a methods for the suctications are recearing the contaces and to a provide of the presention plarable matere indetion ardent include of the recorded to present of the direction of the presenting there of the mortion to the fartication of a seried to at a metal a perfice to and of the componation of the subfection to a sead a component and the first mead controlsimetion of the present in\n",
      "epoch 4\n",
      "training...\n",
      "Seed text: \"evel of the signal lines. Said high permittivity material is\"\n",
      "evel of the signal lines. Said high permittivity material is inconding to the perponent a polymert of the controller for intertable provers of the present in entation is at a method by a control are second by which which a substrating the second device for the present invention is the stating processing a contact of the detecting a control of the frequent of the reaction where and a control and a side according an a control controller between a material to\n",
      "epoch 5\n",
      "training...\n",
      "Seed text: \" and which is stable in a dilute aqueous solution at a pH fo\"\n",
      " and which is stable in a dilute aqueous solution at a pH formed in the detector in the second are selection in a process in the connected to proved be formation. The first elements one or sensition of a second with the contact power component and the oxide and defined in the present invention is provided by encording and one of the present invention and the present invention is output to the relation as also contious out under to provide and the present i\n",
      "epoch 6\n",
      "training...\n",
      "Seed text: \"detector” (PSD) for the commonly used larger arrays. The adv\"\n",
      "detector” (PSD) for the commonly used larger arrays. The advantages of a control and alkotaling composing a first meass of the has a conventional systemment of the seach to the convention of the invention description of the second in a polymers to control mist decond convention sensor to optical signal systems, wherein the second by the optical system with the process that and include and an invention is provided in the first component invention data in an\n",
      "epoch 7\n",
      "training...\n",
      "Seed text: \"ations effective for inducing a perception of enhanced energ\"\n",
      "ations effective for inducing a perception of enhanced energy, a compound for contact with the signal and the charate of the configured to the communication of the device and the stand of size of the drawing may be the standing containing the second to a not one or more of the desired to the system in the amplitude another desired in the second an entages to the specifical field. The second side in the actions of the statist signal of the pirclation of the\n",
      "epoch 8\n",
      "training...\n"
     ]
    }
   ],
   "source": [
    "## This part of code is mostly copied from 2017 François Chollet:\n",
    "## https://github.com/fchollet/deep-learning-with-python-notebooks/\n",
    "\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 25):\n",
    "    print('epoch', epoch)\n",
    "    print('training...')\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1, verbose=0)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('Seed text: \"' + generated_text + '\"')\n",
    "\n",
    "    sys.stdout.write(generated_text)\n",
    "\n",
    "    # We generate 400 characters\n",
    "    for i in range(400):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, 0.5)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
